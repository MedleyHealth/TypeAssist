{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TypeAssist-Training.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMV24FaVpRdY6PjXGMtBOZz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MedleyHealth/TypeAssist/blob/master/TypeAssist_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "179maOLRNVcK",
        "colab_type": "text"
      },
      "source": [
        "# **Important: Do not save the output from code cells in this notebook to Github (or any other public location). Access to the dataset is restricted and we cannot leak any information about individual samples.**\n",
        "\n",
        "To suppress the output in Google Colab:\n",
        "\n",
        "1. Go to Edit > Notebook Settings\n",
        "2. Make sure the checkbox is ticked for \"Omit code cell output when saving this notebook\"\n",
        "\n",
        "# **If you have any doubts about what this means, message me first before committing.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7zOLwPlJq_E",
        "colab_type": "text"
      },
      "source": [
        "### Modified from [code](https://nbviewer.jupyter.org/github/PrithivirajDamodaran/NLP-Experiments/blob/master/Gmail_style_smart_compose_with_char_ngram_based_language_model.ipynb) created by [Prithivi Da](https://github.com/PrithivirajDamodaran)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo_mZJwRNXmn",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AENHqAEUOXHN",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries and set seeds (must use Tensorflow 1.x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOejHYgt_0U7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, CuDNNLSTM, Embedding, Flatten, TimeDistributed, Dropout, LSTMCell, RNN, Bidirectional, Concatenate, Layer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model\n",
        "from datetime import datetime\n",
        "\n",
        "import random\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import string\n",
        "import os \n",
        "\n",
        "seed = 23\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW_oNNe-OYox",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive where dataset is saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5G_v6QLT8k3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiomWx6YOd5n",
        "colab_type": "text"
      },
      "source": [
        "### Load dataset from path in Google Drive (change path to your location)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cj9Bgc3T-fV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/4 Archive/MIMIC/NOTEEVENTS.csv'\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "df[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGfmFP7dxCxJ",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y25WeFeBZpO",
        "colab_type": "text"
      },
      "source": [
        "### Select notes that are less than 100 characters long"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFNVPeh6xJAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [note for note in df['TEXT'] if len(note) < 100]\n",
        "\n",
        "print('Number of Notes with Length < 100:', len(corpus), '\\n')\n",
        "corpus[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqUl-zVQBitc",
        "colab_type": "text"
      },
      "source": [
        "### Split notes on newline characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWByIkj3yDcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [note.split('\\n') for note in corpus]\n",
        "\n",
        "corpus[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtEAYjXgBoT-",
        "colab_type": "text"
      },
      "source": [
        "### Collapse the nested list structure from splitting on newline characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJPgUUXKypJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [split_note for note in corpus for split_note in note if len(split_note) > 10]\n",
        "\n",
        "print('Number of notes after merging sublists:', len(corpus), '\\n')\n",
        "corpus[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6oJ5NW4Bt4K",
        "colab_type": "text"
      },
      "source": [
        "### Drop any notes that contain PHI tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRClYGBuzG-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phi_pattern = '(\\[\\*\\*(.*)\\*\\*\\])'\n",
        "\n",
        "corpus = [note for note in corpus if re.search(phi_pattern, note) is None]\n",
        "\n",
        "print('Number of notes after removing any note that contains a PHI tag:', len(corpus), '\\n')\n",
        "corpus[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOb39w9uePjn",
        "colab_type": "text"
      },
      "source": [
        "### Convert all notes to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYtvhwviLjZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [note.lower() for note in corpus]\n",
        "\n",
        "corpus[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCr_7FSPCmgn",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4RFKRQT_KZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        self.create_index()\n",
        "\n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "        self.vocab = sorted(self.vocab)\n",
        "        self.word2idx[\"<pad>\"] = 0\n",
        "        self.idx2word[0] = \"<pad>\"\n",
        "        for i,word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = i + 1\n",
        "            self.idx2word[i+1] = word\n",
        "\n",
        "\n",
        "def max_length(t):\n",
        "    return max(len(i) for i in t)\n",
        "\n",
        "\n",
        "def clean_special_chars(text):\n",
        "    punct='#$%&*+-/<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "    for p in punct:\n",
        "        text = text.replace(p, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def generate_ngrams(corpus):\n",
        "    processed_corpus = [clean_special_chars(line) for line in corpus]\n",
        "    output = []\n",
        "    for token_list in processed_corpus:\n",
        "        for i in range(1, len(token_list)):\n",
        "            x_ngram = '<start> ' + token_list[:i+1] + ' <end>'\n",
        "            y_ngram = '<start> ' + token_list[i+1:] + ' <end>'\n",
        "            output.append([x_ngram, y_ngram]) \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg6v1aczI40o",
        "colab_type": "text"
      },
      "source": [
        "### Generate n-gram pairs with prefixes and suffixes for teacher forcing technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlxanDbbIi_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = generate_ngrams(corpus)\n",
        "\n",
        "dummy_df = pd.DataFrame(pairs, columns=['input (i)','output (o)'])\n",
        "print('Shape of n-gram pairs: {}\\n'.format(dummy_df.shape))\n",
        "dummy_df[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRihntexL9gs",
        "colab_type": "text"
      },
      "source": [
        "### Convert words to index integers for input / output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7wzZ_PkL7mB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_lang = LanguageIndex(o for i, o in pairs)\n",
        "in_lang = LanguageIndex(i for i, o in pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE6GMI-1MLDO",
        "colab_type": "text"
      },
      "source": [
        "### Generate word embeddings for input / output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAVp4odXMKZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = [[in_lang.word2idx[word] for word in i.split(' ')] for i, o in pairs]\n",
        "output_data = [[out_lang.word2idx[word] for word in o.split(' ')] for i, o in pairs]\n",
        "\n",
        "print('input_data:', input_data[0])\n",
        "print('output_data:', output_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I7adoyQPaWt",
        "colab_type": "text"
      },
      "source": [
        "### Calculate the max length of tokens for input and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4A6lxE2OiOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen_in = max_length(input_data)\n",
        "maxlen_out = max_length(output_data)\n",
        "\n",
        "print('maxlen_in:', maxlen_in)\n",
        "print('maxlen_out:', maxlen_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjigk49sPd4O",
        "colab_type": "text"
      },
      "source": [
        "### Add padding to the input and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBurnHy8M0V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = pad_sequences(input_data, maxlen=maxlen_in, padding=\"post\")\n",
        "output_data = pad_sequences(output_data, maxlen=maxlen_out, padding=\"post\")\n",
        "\n",
        "print('input_data (padded):', input_data[0], '\\n')\n",
        "print('output_data (padded):', output_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK_jatyC89xC",
        "colab_type": "text"
      },
      "source": [
        "### Create target data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8h4JZyj4-HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_data = [[output_data[n][i+1] for i in range(len(output_data[n])-1)] for n in range(len(output_data))]\n",
        "target_data = pad_sequences(target_data, maxlen=maxlen_out, padding=\"post\")\n",
        "\n",
        "print('target_data:', target_data[:3])\n",
        "print('target_data (padded:', target_data[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3ky45tx_TV5",
        "colab_type": "text"
      },
      "source": [
        "### Reshape target_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV4cR-AP_SpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_shape = (target_data.shape[0], target_data.shape[1], 1)\n",
        "print('Using target shape:', target_shape)\n",
        "\n",
        "target_data = target_data.reshape(target_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_quw3j-t_Jjh",
        "colab_type": "text"
      },
      "source": [
        "### Shuffle the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ptEcWP8_Jsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = np.random.permutation(len(input_data))\n",
        "\n",
        "input_data = input_data[p]\n",
        "output_data = output_data[p]\n",
        "target_data = target_data[p]\n",
        "\n",
        "print('input_data:', input_data)\n",
        "print('output_data:', output_data)\n",
        "print('target_data:', target_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXEOVEhp0mXG",
        "colab_type": "text"
      },
      "source": [
        "### Configuration parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1OQjAzK611B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_data)\n",
        "BATCH_SIZE = 128\n",
        "embedding_dim = 300\n",
        "units = 128\n",
        "vocab_in_size = len(in_lang.word2idx)\n",
        "vocab_out_size = len(out_lang.word2idx)\n",
        "loss = 'sparse_categorical_crossentropy'\n",
        "metrics = ['sparse_categorical_accuracy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z65NY6IU0oYs",
        "colab_type": "text"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO4yCHXN7iPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the Encoder layers first.\n",
        "encoder_inputs = Input(shape=(maxlen_in,))\n",
        "encoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
        "\n",
        "# Use this if you dont need Bidirectional LSTM\n",
        "# encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
        "# encoder_out, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
        "\n",
        "encoder_lstm = Bidirectional(CuDNNLSTM(units=units, return_sequences=True, return_state=True))\n",
        "encoder_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
        "state_h = Concatenate()([fstate_h,bstate_h])\n",
        "state_c = Concatenate()([bstate_h,bstate_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Now create the Decoder layers.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
        "decoder_lstm = CuDNNLSTM(units=units*2, return_sequences=True, return_state=True)\n",
        "decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
        "\n",
        "# Two dense layers added to this model to improve inference capabilities.\n",
        "decoder_d1 = Dense(units, activation=\"relu\")\n",
        "decoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
        "decoder_out = decoder_d2(Dropout(rate=.2)(decoder_d1(Dropout(rate=.2)(decoder_lstm_out))))\n",
        "\n",
        "# Finally, create a training model which combines the encoder and the decoder.\n",
        "# Note that this model has three inputs:\n",
        "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_out)\n",
        "\n",
        "opt = tf.train.AdamOptimizer()\n",
        "\n",
        "# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n",
        "model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtnVKzzv0thn",
        "colab_type": "text"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b25pCbWI7nG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "N = 100000\n",
        "\n",
        "history = model.fit([input_data[:N], output_data[:N]], target_data[:N],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQwZq-Ed00H9",
        "colab_type": "text"
      },
      "source": [
        "### Plot training vs. validation loss for signs of overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey-ray5o8pZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'], label=\"Training loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"Validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gb1niLf053-",
        "colab_type": "text"
      },
      "source": [
        "### Create encoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xbhHIls8reI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the encoder model from the tensors we previously declared.\n",
        "encoder_model = Model(encoder_inputs, [encoder_out, state_h, state_c])\n",
        "\n",
        "# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n",
        "# this does not preclude using the same underlying layers that we trained on. (e.g. weights/biases).\n",
        "inf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n",
        "\n",
        "# We'll need to force feed the two state variables into the decoder each step.\n",
        "state_input_h = Input(shape=(units*2,), name=\"state_input_h\")\n",
        "state_input_c = Input(shape=(units*2,), name=\"state_input_c\")\n",
        "\n",
        "decoder_res, decoder_h, decoder_c = decoder_lstm(\n",
        "    decoder_emb(inf_decoder_inputs), \n",
        "    initial_state=[state_input_h, state_input_c])\n",
        "\n",
        "inf_decoder_out = decoder_d2(decoder_d1(decoder_res))\n",
        "\n",
        "inf_model = Model(inputs=[inf_decoder_inputs, state_input_h, state_input_c], \n",
        "                  outputs=[inf_decoder_out, decoder_h, decoder_c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHFBCwfF5J2F",
        "colab_type": "text"
      },
      "source": [
        "### Methods for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyBX54qc8tX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_vector(sentence, lang):\n",
        "    \"\"\"\n",
        "    Converts the given sentence (just a string) into a vector of word IDs\n",
        "    Output is 1-D: [timesteps/words]\n",
        "    \"\"\"\n",
        "\n",
        "    pre = sentence\n",
        "    vec = np.zeros(maxlen_in)\n",
        "    sentence_list = [lang.word2idx[s] for s in pre.split(' ')]\n",
        "    for i,w in enumerate(sentence_list):\n",
        "        vec[i] = w\n",
        "    return vec\n",
        "\n",
        "def translate(input_sentence, infenc_model, infmodel):\n",
        "    \"\"\"\n",
        "    Given an input string, an encoder model (infenc_model) \n",
        "    and a decoder model (infmodel).\n",
        "    \"\"\"\n",
        "\n",
        "    sv = sentence_to_vector(input_sentence, in_lang)\n",
        "    sv = sv.reshape(1,len(sv))\n",
        "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
        "    \n",
        "    i = 0\n",
        "    start_vec = out_lang.word2idx[\"<start>\"]\n",
        "    stop_vec = out_lang.word2idx[\"<end>\"]\n",
        "    \n",
        "    cur_vec = np.zeros((1,1))\n",
        "    cur_vec[0,0] = start_vec\n",
        "    cur_word = \"<start>\"\n",
        "    output_sentence = \"\"\n",
        "\n",
        "    while cur_word != \"<end>\" and i < (maxlen_out-1):\n",
        "        i += 1\n",
        "        if cur_word != \"<start>\":\n",
        "            output_sentence = output_sentence + \" \" + cur_word\n",
        "        x_in = [cur_vec, sh, sc]\n",
        "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
        "        cur_vec[0,0] = np.argmax(nvec[0,0])\n",
        "        cur_word = out_lang.idx2word[np.argmax(nvec[0,0])]\n",
        "\n",
        "    return output_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f3YKGwQ5FOu",
        "colab_type": "text"
      },
      "source": [
        "### Run tests to see how the model performs (we want inference < 100ms)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15CHnwXF8vxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note that only words that we've trained the model on will be available, otherwise you'll get an error.\n",
        "\n",
        "test = [\n",
        "  'discha', #arge summary\n",
        "  'left v', #entricular hypertrophy\n",
        "  'no ch', #ange from previous\n",
        "  'ventr', #ricular paced\n",
        "  'no sig', #nificant change\n",
        "  'previ', #ious tracing\n",
        "  'no ma', #ajor change\n",
        "  'sinu', #s rhythm\n",
        "  'R wav', #e progression,\n",
        "  'hydroc', #hlorothiazide\n",
        "]\n",
        "\n",
        "output = []  \n",
        "for t in test:  \n",
        "  input_seq = t.lower()\n",
        "\n",
        "  t0 = datetime.now()\n",
        "  pred_seq = translate(t.lower(), encoder_model, inf_model)\n",
        "  t1 = datetime.now()\n",
        "  print('Inference time:', (t1-t0).total_seconds())\n",
        "\n",
        "  output.append({\"Input Sequence\": input_seq, \"Predicted Sequence\": pred_seq})\n",
        "\n",
        "results_df = pd.DataFrame.from_dict(output) \n",
        "results_df.head(len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e3YP0_X4SaW",
        "colab_type": "text"
      },
      "source": [
        "### Save the model to JSON and weights to H5 (change save_path to your location)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mArZClfFeAOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = '/content/drive/My Drive/3 Reference/TypeAssist/model_1'\n",
        "\n",
        "if os.path.exists('{}.json'.format(save_path)):\n",
        "  raise BaseException('WARNING. Save path exists. Please increment the model number.')\n",
        "\n",
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = inf_model.to_json()\n",
        "\n",
        "with open('{}.json'.format(save_path), 'w') as f:\n",
        "    f.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "inf_model.save_weights('{}.h5'.format(save_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5mh_DiDHVwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}