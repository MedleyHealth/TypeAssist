{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TypeAssist-Training.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJEP98A3bhufIY135FZNzV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MedleyHealth/TypeAssist/blob/master/TypeAssist_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "179maOLRNVcK",
        "colab_type": "text"
      },
      "source": [
        "# **Important: Do not save the output from code cells in this notebook to Github (or any other public location). Access to the dataset is restricted and we cannot leak any information about individual samples. If you have any doubts about what this means, message me first before committing.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7zOLwPlJq_E",
        "colab_type": "text"
      },
      "source": [
        "### Modified from [code](https://nbviewer.jupyter.org/github/PrithivirajDamodaran/NLP-Experiments/blob/master/Gmail_style_smart_compose_with_char_ngram_based_language_model.ipynb) created by [Prithivi Da](https://github.com/PrithivirajDamodaran)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo_mZJwRNXmn",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOejHYgt_0U7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, CuDNNLSTM, Embedding, Flatten, TimeDistributed, Dropout, LSTMCell, RNN, Bidirectional, Concatenate, Layer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import string\n",
        "import os \n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5G_v6QLT8k3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cj9Bgc3T-fV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/4 Archive/MIMIC/NOTEEVENTS.csv'\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "df[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGfmFP7dxCxJ",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4RFKRQT_KZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        self.create_index()\n",
        "\n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "        self.vocab = sorted(self.vocab)\n",
        "        self.word2idx[\"<pad>\"] = 0\n",
        "        self.idx2word[0] = \"<pad>\"\n",
        "        for i,word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = i + 1\n",
        "            self.idx2word[i+1] = word\n",
        "\n",
        "\n",
        "def max_length(t):\n",
        "    return max(len(i) for i in t)\n",
        "\n",
        "\n",
        "def clean_special_chars(text, punct):\n",
        "    for p in punct:\n",
        "        text = text.replace(p, '')\n",
        "    return text\n",
        "\n",
        "      \n",
        "def preprocess(data):\n",
        "    output = []\n",
        "    punct = '#$%&*+-/<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "    for line in data:\n",
        "         pline= clean_special_chars(line.lower(), punct)\n",
        "         output.append(pline)\n",
        "    return output  \n",
        "\n",
        "\n",
        "def generate_dataset(corpus):\n",
        "  \n",
        "    processed_corpus = preprocess(corpus)    \n",
        "    output = []\n",
        "    for line in processed_corpus:\n",
        "        token_list = line\n",
        "        for i in range(1, len(token_list)):\n",
        "            data = []\n",
        "            x_ngram = '<start> '+ token_list[:i+1] + ' <end>'\n",
        "            y_ngram = '<start> '+ token_list[i+1:] + ' <end>'\n",
        "            data.append(x_ngram)\n",
        "            data.append(y_ngram)\n",
        "            output.append(data)\n",
        "    print(\"Dataset prepared with prefix and suffixes for teacher forcing technique\")\n",
        "    dummy_df = pd.DataFrame(output, columns=['input','output'])\n",
        "    return output, dummy_df     \n",
        "\n",
        "\n",
        "def load_dataset(corpus):\n",
        "    pairs, df = generate_dataset(corpus)\n",
        "    out_lang = LanguageIndex(sp for en, sp in pairs)\n",
        "    in_lang = LanguageIndex(en for en, sp in pairs)\n",
        "    input_data = [[in_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
        "    output_data = [[out_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
        "\n",
        "    max_length_in, max_length_out = max_length(input_data), max_length(output_data)\n",
        "    input_data = pad_sequences(input_data, maxlen=max_length_in, padding=\"post\")\n",
        "    output_data = pad_sequences(output_data, maxlen=max_length_out, padding=\"post\")\n",
        "    return input_data, output_data, in_lang, out_lang, max_length_in, max_length_out, df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFNVPeh6xJAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [note for note in df['TEXT'] if len(note) < 100]\n",
        "\n",
        "print('Number of Notes with Length < 100:', len(corpus), '\\n')\n",
        "corpus[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWByIkj3yDcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_split = [note.split('\\n') for note in corpus]\n",
        "\n",
        "corpus_split[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJPgUUXKypJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_merge = [split_note for note in corpus_split for split_note in note if len(split_note) > 10]\n",
        "\n",
        "print('Number of notes after merging sublists:', len(corpus_merge), '\\n')\n",
        "\n",
        "corpus_merge[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRClYGBuzG-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_scrubbed = [note for note in corpus_merge if re.search('(\\[\\*\\*(.*)\\*\\*\\])', note) is None]\n",
        "\n",
        "print('Number of notes after removing any note that contains a PHI tag:', len(corpus_scrubbed), '\\n')\n",
        "\n",
        "corpus_scrubbed[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8h4JZyj4-HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data, teacher_data, input_lang, target_lang, len_input, len_target, df = load_dataset(corpus_scrubbed)\n",
        "\n",
        "target_data = [[teacher_data[n][i+1] for i in range(len(teacher_data[n])-1)] for n in range(len(teacher_data))]\n",
        "target_data = pad_sequences(target_data, maxlen=len_target, padding=\"post\")\n",
        "\n",
        "\n",
        "target_shape = (target_data.shape[0], target_data.shape[1], 1)\n",
        "print('Using target shape:', target_shape)\n",
        "\n",
        "target_data = target_data.reshape(target_shape)\n",
        "\n",
        "# Shuffle all of the data in unison. This training set has the longest (e.g. most complicated) data at the end,\n",
        "# so a simple Keras validation split will be problematic if not shuffled.\n",
        "\n",
        "p = np.random.permutation(len(input_data))\n",
        "input_data = input_data[p]\n",
        "teacher_data = teacher_data[p]\n",
        "target_data = target_data[p]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1OQjAzK611B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "BUFFER_SIZE = len(input_data)\n",
        "BATCH_SIZE = 128\n",
        "embedding_dim = 300\n",
        "units = 128\n",
        "vocab_in_size = len(input_lang.word2idx)\n",
        "vocab_out_size = len(target_lang.word2idx)\n",
        "df.iloc[0:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q81odzT7T5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO4yCHXN7iPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the Encoder layers first.\n",
        "encoder_inputs = Input(shape=(len_input,))\n",
        "encoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
        "\n",
        "# Use this if you dont need Bidirectional LSTM\n",
        "# encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
        "# encoder_out, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
        "\n",
        "encoder_lstm = Bidirectional(CuDNNLSTM(units=units, return_sequences=True, return_state=True))\n",
        "encoder_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
        "state_h = Concatenate()([fstate_h,bstate_h])\n",
        "state_c = Concatenate()([bstate_h,bstate_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Now create the Decoder layers.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
        "decoder_lstm = CuDNNLSTM(units=units*2, return_sequences=True, return_state=True)\n",
        "decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
        "\n",
        "# Two dense layers added to this model to improve inference capabilities.\n",
        "decoder_d1 = Dense(units, activation=\"relu\")\n",
        "decoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
        "decoder_out = decoder_d2(Dropout(rate=.2)(decoder_d1(Dropout(rate=.2)(decoder_lstm_out))))\n",
        "\n",
        "\n",
        "# Finally, create a training model which combines the encoder and the decoder.\n",
        "# Note that this model has three inputs:\n",
        "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_out)\n",
        "\n",
        "opt = tf.train.AdamOptimizer()\n",
        "\n",
        "# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n",
        "model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b25pCbWI7nG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "N = 100000\n",
        "\n",
        "history = model.fit([input_data[:N], teacher_data[:N]], target_data[:N],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey-ray5o8pZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'], label=\"Training loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"Validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xbhHIls8reI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the encoder model from the tensors we previously declared.\n",
        "encoder_model = Model(encoder_inputs, [encoder_out, state_h, state_c])\n",
        "\n",
        "# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n",
        "# this does not preclude using the same underlying layers that we trained on. (e.g. weights/biases).\n",
        "\n",
        "inf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n",
        "# We'll need to force feed the two state variables into the decoder each step.\n",
        "state_input_h = Input(shape=(units*2,), name=\"state_input_h\")\n",
        "state_input_c = Input(shape=(units*2,), name=\"state_input_c\")\n",
        "decoder_res, decoder_h, decoder_c = decoder_lstm(\n",
        "    decoder_emb(inf_decoder_inputs), \n",
        "    initial_state=[state_input_h, state_input_c])\n",
        "inf_decoder_out = decoder_d2(decoder_d1(decoder_res))\n",
        "inf_model = Model(inputs=[inf_decoder_inputs, state_input_h, state_input_c], \n",
        "                  outputs=[inf_decoder_out, decoder_h, decoder_c])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyBX54qc8tX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Converts the given sentence (just a string) into a vector of word IDs\n",
        "# Output is 1-D: [timesteps/words]\n",
        "\n",
        "def sentence_to_vector(sentence, lang):\n",
        "\n",
        "    pre = sentence\n",
        "    vec = np.zeros(len_input)\n",
        "    sentence_list = [lang.word2idx[s] for s in pre.split(' ')]\n",
        "    for i,w in enumerate(sentence_list):\n",
        "        vec[i] = w\n",
        "    return vec\n",
        "\n",
        "# Given an input string, an encoder model (infenc_model) and a decoder model (infmodel),\n",
        "def translate(input_sentence, infenc_model, infmodel):\n",
        "    sv = sentence_to_vector(input_sentence, input_lang)\n",
        "    sv = sv.reshape(1,len(sv))\n",
        "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
        "    \n",
        "    i = 0\n",
        "    start_vec = target_lang.word2idx[\"<start>\"]\n",
        "    stop_vec = target_lang.word2idx[\"<end>\"]\n",
        "    \n",
        "    cur_vec = np.zeros((1,1))\n",
        "    cur_vec[0,0] = start_vec\n",
        "    cur_word = \"<start>\"\n",
        "    output_sentence = \"\"\n",
        "\n",
        "    while cur_word != \"<end>\" and i < (len_target-1):\n",
        "        i += 1\n",
        "        if cur_word != \"<start>\":\n",
        "            output_sentence = output_sentence + \" \" + cur_word\n",
        "        x_in = [cur_vec, sh, sc]\n",
        "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
        "        cur_vec[0,0] = np.argmax(nvec[0,0])\n",
        "        cur_word = target_lang.idx2word[np.argmax(nvec[0,0])]\n",
        "    return output_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15CHnwXF8vxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Note that only words that we've trained the model on will be available, otherwise you'll get an error.\n",
        "\n",
        "\n",
        "test = [\n",
        "  'discha', #arge summary\n",
        "  'left v', #entricular hypertrophy\n",
        "  'no ch', #ange from previous\n",
        "  'ventr', #ricular paced\n",
        "  'no sig', #nificant change\n",
        "  'previ', #ious tracing\n",
        "  'no ma', #ajor change\n",
        "  'sinu', #s rhythm\n",
        "  'R wav', #e progression,\n",
        "  'hydroc', #hlorothiazide\n",
        "]\n",
        "\n",
        "output = []  \n",
        "for t in test:  \n",
        "  output.append({\"Input seq\":t.lower(), \"Pred. Seq\":translate(t.lower(), encoder_model, inf_model)})\n",
        "\n",
        "results_df = pd.DataFrame.from_dict(output) \n",
        "results_df.head(len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mArZClfFeAOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "#  the keras model which is trained is defined as 'model' in this example\n",
        "model_json = inf_model.to_json()\n",
        "\n",
        "with open(\"./sample_data/model_num.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "inf_model.save_weights(\"./sample_data/model_num.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}